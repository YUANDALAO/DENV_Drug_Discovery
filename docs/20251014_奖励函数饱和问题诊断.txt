日期: 2025.10.14
核心主题: 生成式AI优化与奖励函数工程
1. 强化学习奖励函数饱和问题诊断:

问题发现: 在REINVENT4的LibInvent生成式模型训练中，发现生成分子的预测pIC50高度集中在7.5-7.7区间（98.6%），Total Score在200步后即达到平台期（0.60-0.65），无法进一步优化至目标值（0.85）。
根因分析: QSAR模型（Random Forest, R²=0.69）的训练集偏向中等活性分子（均值pIC50=7.35），导致对新颖结构的预测回归至训练集均值附近，且原始pIC50作为奖励信号时，7.5→8.0的差异（Δ=0.05）在几何平均后被进一步压缩（ΔTotal≈0.014），不足以驱动模型持续探索更高活性空间。

2. Sigmoid Transform奖励塑形 (Reward Shaping):

理论依据: 在强化学习中，奖励函数的梯度决定了策略优化的方向和幅度。对于窄分布的QSAR预测（7.0-8.0），线性映射导致奖励信号过于平坦，无法有效区分"好"与"更好"的分子。Sigmoid变换通过非线性映射，在目标区域（7.5-9.0）放大梯度，将pIC50 7.5→8.0的奖励差异从0.05扩大至0.23（4.6倍），显著提升了优化敏感度。
实现方案: 采用两阶段Sigmoid变换策略：

Stage 1 (探索阶段): transform.type = "sigmoid", low=7.0, high=8.5, k=2.0 —— 鼓励从中等活性（7.0-7.5）向高活性（8.0-8.5）过渡
Stage 2 (优化阶段): low=7.5, high=9.0, k=2.5 —— 提高标准，聚焦极高活性（>8.5），更陡峭的曲线加速收敛


数学公式:
$\text{Score}_{\text{transformed}} = \frac{1}{1 + e^{-k \cdot (\text{pIC50} - \frac{\text{low}+\text{high}}{2})}}


3. 学习率与Sigma参数协同调优:

学习率降低 (0.0001 → 0.00003): 较小的学习率配合重塑后的陡峭奖励曲线，防止模型在高梯度区域产生过度更新，确保稳定探索pIC50>8.5的罕见化学空间。
Sigma降低 (120 → 60): 在DAP (Diversity-Augmented Policy) 算法中，sigma控制奖励的温度。降低sigma使模型对高分更敏感（奖励函数从近线性变为指数衰减），配合Sigmoid变换形成"双重放大"机制，有效打破早期收敛陷阱。
协同效应: Sigmoid拉伸分数空间 + 低学习率稳定更新 + 低sigma强化高分偏好 = 持续、稳定地推动活性边界（pIC50从7.5→8.5→9.0）。

4. 预期科学影响:

该优化策略解决了"QSAR模型作为黑盒奖励函数时，因训练集偏差导致生成模型早期收敛"的普遍问题，为基于ML模型的分子生成提供了可复用的奖励工程范式。