# 机器学习的哲学与实践：从知识到优化
## 机器必须学习吗？
### 机器学习的两个层面
#### 离线学习（已完成的学习）
- python# QSAR模型 - 已经从历史数据学到的知识
- model_path = "random_forest_champion.joblib"
- prior_file = "denv_ultra_clean_model.model"
#### 在线学习（正在进行的学习）
- toml[learning_strategy]
- type = "dap"  # 通过强化学习持续优化
- sigma = 120
- rate = 0.0001
#### 哲学答案
- 哲学答案：机器不必"持续学习"，但必须基于某种已有知识。
- 你的系统即使不做RL训练，仅用预训练的QSAR模型+规则也能工作——但效果会受限。
## 学习的榜样是什么？
### 三层榜样结构
#### 客观规律（物理真理）
- QSAR模型预测的pIC50（生物活性）自然规律不可违背
#### 主观经验（人类经验）
- QED、Lipinski规则、合成可及性前人智慧的结晶
#### 负面案例（自我约束）
- CustomAlerts的SMARTS规则避免已知陷阱
#### 哲学核心
- 哲学核心：机器学习的"榜样"不是单一的，而是：
- 客观规律（实验测得的活性）
- 主观经验（药物化学家的经验法则）
- 负面案例（需要避免的结构）
## 从哪里学习？
### 知识的三个来源
#### 显式数据（你已提供的）
- toml# 活性数据的"影子" - QSAR模型
- model_path = "random_forest_champion.joblib"
- prior_file = "denv_ultra_clean_model.model"
- smiles_file = "denv_ultra_clean.tsv"
#### 隐式知识（编码在规则中）
- toml# 类药性的集体智慧
- [scoring.component.Qed]  # 基于FDA批准药物统计
- [scoring.component.SAScore]
#### 探索产生的新知识
- toml[diversity_filter]
- type = "IdenticalMurckoScaffold"  # 强制探索不同骨架
## 没有活性数据时，机器能学什么？
### 极端情况下的系统表现
#### 删除QSAR组件后的配置
- toml# 仅用规则引导的版本
- [stage.scoring]
- type = "geometric_mean"
- [[stage.scoring.component]]
- [stage.scoring.component.Qed]  # 类药性（基于统计）
- weight = 1.0
- [[stage.scoring.component]]
- [stage.scoring.component.SlogP]  # LogP优化
- weight = 0.8
- [[stage.scoring.component]]
- [stage.scoring.component.CustomAlerts]  # 避免坏结构
- weight = 1.0
#### 机器学习的内容
- 此时机器学习的是：
- 化学空间的语法（从prior model）
- 统计规律（QED是基于已知药物的分布）
- 约束满足（Lipinski规则、分子量范围）
## 哲学核心：学习的本质是什么？
### 三种知识论
#### 经验主义（Empiricism）
- python# QSAR模型 = 从实验数据归纳
- model.predict(fingerprint) → pIC50
#### 理性主义（Rationalism）
- toml# 药物化学规则 = 演绎推理
- transform.type = "double_sigmoid"
- low = 250.0  # 分子量过小难成药
- high = 500.0  # 分子量过大难吸收
#### 进化论（Evolutionary）
- toml[learning_strategy]
- type = "dap"  # 通过试错进化
- [diversity_filter]  # 多样性产生创新
## 实践中的启示
### 场景1：完全没有活性数据
#### 可行策略
- toml# 用规则驱动 + 结构多样性
- [[stage.scoring.component]]
- [stage.scoring.component.TanimotoDistance]
- params.smiles = ["已知弱活性化合物"]  # 相似性种子
- [[stage.scoring.component]]
- [stage.scoring.component.MatchingSubstructure]
- params.smarts = "药效团SMARTS"  # 基于靶点结构的理性设计
### 场景2：只有少量活性数据
#### 当前策略（最优）
- QSAR模型捕获活性趋势
- 物理化学规则防止过拟合
- 多样性过滤探索新空间
### 场景3：活性数据充足
#### 优化策略
- toml[[stage.scoring.component.QSARScorer.endpoint]]
- weight = 2.0  # 更信任数据
## 哲学结论
### 机器学习的三个真相
#### 不存在"从零学习"
- 所有学习都基于先验（prior）——你的prior_file就是语言模型，QSAR模型是经验知识
#### 榜样是多元的
- 生物活性、类药性、合成性、多样性——没有单一"正确答案"
#### 学习≠记忆
- 你的RL不是记住训练集，而是学习在约束下优化的策略（exploration vs exploitation）
#### 最深刻的洞察
- 机器学习本质是在人类定义的目标函数下，搜索可能性空间。
- 你的geometric_mean多目标优化就是这种哲学的完美体现——没有绝对真理，只有权衡（trade-offs）。