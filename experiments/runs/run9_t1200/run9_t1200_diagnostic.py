"""
Run9_t1200 ‰∏ìÁî®ËØäÊñ≠ËÑöÊú¨
ÈÄÇÁî®Âüü(AD)ÂàÜÊûê - Ê£ÄÊµãÁîüÊàêÂàÜÂ≠êÊòØÂê¶ÂèØÈù†
"""

import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import os

# ============================================================================
# ÈÖçÁΩÆÂå∫ - Ê†πÊçÆÊÇ®ÁöÑÂÆûÈôÖÊÉÖÂÜµÂ∑≤ÈÖçÁΩÆÂ•Ω
# ============================================================================

# ËÆ≠ÁªÉÈõÜÊñá‰ª∂ÔºàÂÆûÈôÖ‰ΩøÁî®ÁöÑÊ∏ÖÊ¥ÅÁâàÊú¨Ôºâ
TRAINING_FILE = "../../../data/denv_ultra_clean.tsv"  # Â¶ÇÊûúËøô‰∏™Êñá‰ª∂‰∏çÂ≠òÂú®Ôºå‰ºöÂ∞ùËØïNS3.csv

# Â§áÈÄâËÆ≠ÁªÉÈõÜÔºàÂéüÂßãÊï∞ÊçÆÔºâ
BACKUP_TRAINING_FILE = "../../../data/NS3.csv"

# ÁîüÊàêÂàÜÂ≠êÊñá‰ª∂
GENERATED_FILE = "results_1.csv"

# ËæìÂá∫ÁõÆÂΩï
OUTPUT_DIR = "diagnostic_results"

# È°πÁõÆÂêçÁß∞
PROJECT_NAME = "Run9_t1200 - DENV NS3 Inhibitor Generation"

# ============================================================================
# Êï∞ÊçÆËØªÂèñÂáΩÊï∞
# ============================================================================

def load_training_data():
    """Êô∫ËÉΩÂä†ËΩΩËÆ≠ÁªÉÈõÜÊï∞ÊçÆ"""
    
    print("Ê≠£Âú®Âä†ËΩΩËÆ≠ÁªÉÈõÜ...")
    
    # Â∞ùËØïÂä†ËΩΩÊ∏ÖÊ¥ÅÁâàÊú¨
    if os.path.exists(TRAINING_FILE):
        print(f"‚úì ÊâæÂà∞Ê∏ÖÊ¥ÅËÆ≠ÁªÉÈõÜ: {TRAINING_FILE}")
        try:
            # TSVÊñá‰ª∂ÔºåÊ≤°Êúâheader
            with open(TRAINING_FILE, 'r') as f:
                smiles_list = [line.strip() for line in f if line.strip()]
            print(f"  Âä†ËΩΩ‰∫Ü {len(smiles_list)} ‰∏™SMILES")
            return smiles_list
        except Exception as e:
            print(f"  ËØªÂèñÂ§±Ë¥•: {e}")
    
    # Â∞ùËØïÂä†ËΩΩÂéüÂßãCSV
    print(f"Â∞ùËØïÂ§áÈÄâÊñá‰ª∂: {BACKUP_TRAINING_FILE}")
    try:
        df = pd.read_csv(BACKUP_TRAINING_FILE)
        
        # Â∞ùËØï‰∏çÂêåÁöÑÂàóÂêç
        smiles_col = None
        for col in ['Smiles', 'SMILES', 'smiles', 'canonical_smiles']:
            if col in df.columns:
                smiles_col = col
                break
        
        if smiles_col is None:
            print(f"‚úó ÈîôËØØ: Êâæ‰∏çÂà∞SMILESÂàóÔºåÂèØÁî®Âàó: {df.columns.tolist()}")
            return None
        
        smiles_list = df[smiles_col].dropna().tolist()
        print(f"‚úì ‰ªé {BACKUP_TRAINING_FILE} Âä†ËΩΩ‰∫Ü {len(smiles_list)} ‰∏™SMILES")
        
        # Â∫îÁî®Ë∂Ö‰∏•Ê†ºÁ≠õÈÄâÔºà‰∏éÊÇ®ÁöÑcreate_ultra_clean_dataset.py‰∏ÄËá¥Ôºâ
        print("  Â∫îÁî®Ë∂Ö‰∏•Ê†ºÁ≠õÈÄâ...")
        clean_smiles = apply_ultra_clean_filter(smiles_list)
        print(f"  Á≠õÈÄâÂêé: {len(clean_smiles)} ‰∏™SMILES")
        
        return clean_smiles
        
    except Exception as e:
        print(f"‚úó ËØªÂèñÂ§±Ë¥•: {e}")
        return None


def apply_ultra_clean_filter(smiles_list):
    """Â∫îÁî®‰∏éËÆ≠ÁªÉÊó∂Áõ∏ÂêåÁöÑË∂Ö‰∏•Ê†ºÁ≠õÈÄâ"""
    
    def is_ultra_compatible(smiles):
        forbidden_tokens = ['[n-]', '[s+]', '[N-]', '[S-]', '[O+]', '[C-]', '[c-]']
        
        for token in forbidden_tokens:
            if token in smiles:
                return False
        
        if any(x in smiles for x in ['[nH+]', '[s+]', '[o+]']):
            return False
        
        if any(x in smiles for x in ['@', '/', '\\']):
            return False
        
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is None:
                return False
            
            if mol.GetNumAtoms() > 50:
                return False
            
            atoms = set([atom.GetSymbol() for atom in mol.GetAtoms()])
            allowed_atoms = {'C', 'N', 'O', 'S', 'F', 'Cl', 'Br', 'H'}
            if not atoms.issubset(allowed_atoms):
                return False
            
            return True
        except:
            return False
    
    valid_smiles = []
    
    for smiles in tqdm(smiles_list, desc="Á≠õÈÄâËÆ≠ÁªÉÈõÜ"):
        if pd.isna(smiles):
            continue
        
        smiles_str = str(smiles).strip()
        
        try:
            mol = Chem.MolFromSmiles(smiles_str)
            if mol is not None:
                canonical_smiles = Chem.MolToSmiles(mol)
                
                if ('.' not in canonical_smiles and 
                    '|' not in canonical_smiles and
                    len(canonical_smiles) < 100 and
                    is_ultra_compatible(canonical_smiles)):
                    valid_smiles.append(canonical_smiles)
        except:
            continue
    
    # ÂéªÈáç
    unique_smiles = list(dict.fromkeys(valid_smiles))
    return unique_smiles


def load_generated_data():
    """Âä†ËΩΩÁîüÊàêÁöÑÂàÜÂ≠êÊï∞ÊçÆ"""
    
    print(f"\nÊ≠£Âú®Âä†ËΩΩÁîüÊàêÂàÜÂ≠ê: {GENERATED_FILE}")
    
    try:
        # ÊòéÁ°ÆÊåáÂÆöÈÄóÂè∑ÂàÜÈöî
        df = pd.read_csv(GENERATED_FILE, sep=',')
        
        print(f"  Êñá‰ª∂ÂΩ¢Áä∂: {df.shape}")
        print(f"  Ââç5Âàó: {df.columns.tolist()[:5]}")
        
        # Áõ¥Êé•‰ΩøÁî®SMILESÂàó
        if 'SMILES' not in df.columns:
            print(f"  ‚úó ÈîôËØØ: Êú™ÊâæÂà∞SMILESÂàó")
            print(f"  ÂèØÁî®Âàó: {df.columns.tolist()}")
            return None, None
        
        smiles_list = df['SMILES'].dropna().tolist()
        print(f"‚úì Âä†ËΩΩ‰∫Ü {len(smiles_list)} ‰∏™ÁîüÊàêÂàÜÂ≠ê")
        print(f"  Á¨¨1‰∏™: {smiles_list[0][:60]}...")
        
        # ËØªÂèñÈ¢ùÂ§ñ‰ø°ÊÅØ
        extra_cols = {}
        if 'Score' in df.columns:
            extra_cols['total_score'] = df['Score'].tolist()
            print(f"  ‚úì ÊâæÂà∞ScoreÂàó")
        if 'DENV_Activity (raw)' in df.columns:
            extra_cols['DENV_Activity'] = df['DENV_Activity (raw)'].tolist()
            print(f"  ‚úì ÊâæÂà∞DENV_ActivityÂàó")
        if 'NLL' in df.columns:
            extra_cols['NLL'] = df['NLL'].tolist()
        
        return smiles_list, extra_cols
        
    except Exception as e:
        print(f"‚úó ËØªÂèñÂ§±Ë¥•: {e}")
        import traceback
        traceback.print_exc()
        return None, None


# ============================================================================
# ADÂàÜÊûêÊ†∏ÂøÉÂáΩÊï∞
# ============================================================================

def compute_fingerprints(smiles_list, radius=2, n_bits=2048):
    """ËÆ°ÁÆóMorganÊåáÁ∫π"""
    fps = []
    valid_smiles = []
    
    for smi in tqdm(smiles_list, desc="ËÆ°ÁÆóÊåáÁ∫π"):
        mol = Chem.MolFromSmiles(smi)
        if mol:
            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)
            fps.append(fp)
            valid_smiles.append(smi)
    
    return fps, valid_smiles


def calculate_ad_analysis(training_fps, generated_smiles, generated_fps, extra_cols=None):
    """ËÆ°ÁÆóADÂàÜÊûê"""
    results = []
    
    print("\nËÆ°ÁÆóADË∑ùÁ¶ª...")
    for idx, (smi, qfp) in enumerate(tqdm(zip(generated_smiles, generated_fps), 
                                          total=len(generated_smiles))):
        # ËÆ°ÁÆóÂà∞ÊâÄÊúâËÆ≠ÁªÉÈõÜÂàÜÂ≠êÁöÑTanimotoÁõ∏‰ººÂ∫¶
        similarities = [
            DataStructs.TanimotoSimilarity(qfp, tfp) 
            for tfp in training_fps
        ]
        
        max_sim = np.max(similarities)
        mean_sim = np.mean(similarities)
        
        # ADÂà§ÂÆö
        if max_sim > 0.6:
            status = "Inside (Safe)"
            risk_score = 1
        elif max_sim > 0.4:
            status = "Boundary (Caution)"
            risk_score = 3
        else:
            status = "Outside (High Risk)"
            risk_score = 5
        
        result = {
            'SMILES': smi,
            'MaxTanimoto': max_sim,
            'MeanTanimoto': mean_sim,
            'AD_Status': status,
            'Risk_Score': risk_score
        }
        
        # Ê∑ªÂä†È¢ùÂ§ñ‰ø°ÊÅØ
        if extra_cols:
            if 'total_score' in extra_cols:
                result['Total_Score'] = extra_cols['total_score'][idx]
            if 'NLL' in extra_cols:
                result['NLL'] = extra_cols['NLL'][idx]
        
        results.append(result)
    
    df = pd.DataFrame(results)
    
    # ÁªüËÆ°Êä•Âëä
    print("\n" + "="*70)
    print("ÈÄÇÁî®Âüü(AD)ÂàÜÊûêÊä•Âëä")
    print("="*70)
    print(f"ÊÄªÂàÜÂ≠êÊï∞: {len(df)}")
    print(f"\nADÂàÜÂ∏É:")
    for status in df['AD_Status'].value_counts().items():
        print(f"  {status[0]}: {status[1]} ({status[1]/len(df)*100:.1f}%)")
    print(f"\nÊúÄÂ§ßTanimotoÁªüËÆ°:")
    print(df['MaxTanimoto'].describe())
    
    if 'Total_Score' in df.columns:
        print(f"\nTotal ScoreÁªüËÆ°:")
        print(df['Total_Score'].describe())
        
        # È´òÂàÜ‰ΩÜÈ´òÈ£éÈô©ÁöÑÂàÜÂ≠ê
        high_score_high_risk = df[(df['Total_Score'] > 0.7) & (df['Risk_Score'] >= 3)]
        print(f"\nÈ´òÂàÜ‰ΩÜÈ´òÈ£éÈô©ÂàÜÂ≠ê: {len(high_score_high_risk)} ‰∏™")
    
    print("="*70)
    
    return df


def plot_ad_distribution(df, save_path):
    """ÂèØËßÜÂåñADÂàÜÂ∏É"""
    
    # Âà§Êñ≠ÊòØÂê¶Êúâscore‰ø°ÊÅØ
    has_score = 'Total_Score' in df.columns
    
    if has_score:
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        axes = axes.flatten()
    else:
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
    
    # 1. TanimotoÂàÜÂ∏ÉÁõ¥ÊñπÂõæ
    ax = axes[0]
    ax.hist(df['MaxTanimoto'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')
    ax.axvline(x=0.6, color='green', linestyle='--', linewidth=2, label='Safe (>0.6)')
    ax.axvline(x=0.4, color='orange', linestyle='--', linewidth=2, label='Caution (0.4-0.6)')
    ax.set_xlabel('Max Tanimoto Similarity', fontsize=11, fontweight='bold')
    ax.set_ylabel('Count', fontsize=11, fontweight='bold')
    ax.set_title('AD Distribution', fontsize=12, fontweight='bold')
    ax.legend(fontsize=9)
    ax.grid(alpha=0.3)
    
    # 2. ADÁä∂ÊÄÅÈ•ºÂõæ
    ax = axes[1]
    status_counts = df['AD_Status'].value_counts()
    colors = {'Inside (Safe)': '#2ecc71', 
              'Boundary (Caution)': '#f39c12', 
              'Outside (High Risk)': '#e74c3c'}
    ax.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%',
            colors=[colors.get(s, 'gray') for s in status_counts.index], 
            startangle=90, textprops={'fontsize': 9, 'fontweight': 'bold'})
    ax.set_title('AD Status', fontsize=12, fontweight='bold')
    
    # 3. Á¥ØÁßØÂàÜÂ∏É
    ax = axes[2]
    sorted_sim = np.sort(df['MaxTanimoto'])
    cumulative = np.arange(1, len(sorted_sim) + 1) / len(sorted_sim)
    ax.plot(sorted_sim, cumulative, linewidth=2, color='darkblue')
    ax.axvline(x=0.4, color='orange', linestyle='--', alpha=0.7, linewidth=2)
    ax.axvline(x=0.6, color='green', linestyle='--', alpha=0.7, linewidth=2)
    ax.set_xlabel('Max Tanimoto', fontsize=11, fontweight='bold')
    ax.set_ylabel('Cumulative Probability', fontsize=11, fontweight='bold')
    ax.set_title('Cumulative Distribution', fontsize=12, fontweight='bold')
    ax.grid(alpha=0.3)
    
    # 4. Max vs Mean Tanimoto
    ax = axes[3]
    scatter = ax.scatter(df['MeanTanimoto'], df['MaxTanimoto'], 
                         c=df['MaxTanimoto'], cmap='RdYlGn', 
                         alpha=0.6, s=20, edgecolors='black', linewidth=0.5)
    ax.axhline(y=0.6, color='green', linestyle='--', alpha=0.5, linewidth=2)
    ax.axhline(y=0.4, color='orange', linestyle='--', alpha=0.5, linewidth=2)
    ax.set_xlabel('Mean Tanimoto', fontsize=11, fontweight='bold')
    ax.set_ylabel('Max Tanimoto', fontsize=11, fontweight='bold')
    ax.set_title('Mean vs Max Tanimoto', fontsize=12, fontweight='bold')
    plt.colorbar(scatter, ax=ax, label='Max Tanimoto')
    ax.grid(alpha=0.3)
    
    if has_score:
        # 5. Score vs AD
        ax = axes[4]
        for status in df['AD_Status'].unique():
            subset = df[df['AD_Status'] == status]
            ax.scatter(subset['MaxTanimoto'], subset['Total_Score'], 
                      label=status, alpha=0.6, s=30)
        ax.set_xlabel('Max Tanimoto', fontsize=11, fontweight='bold')
        ax.set_ylabel('Total Score', fontsize=11, fontweight='bold')
        ax.set_title('Score vs AD', fontsize=12, fontweight='bold')
        ax.legend(fontsize=8)
        ax.grid(alpha=0.3)
        
        # 6. Risk ScoreÂàÜÂ∏É
        ax = axes[5]
        risk_counts = df.groupby(['AD_Status', 'Risk_Score']).size().unstack(fill_value=0)
        risk_counts.plot(kind='bar', stacked=True, ax=ax, 
                        color=['#2ecc71', '#f39c12', '#e74c3c'])
        ax.set_xlabel('AD Status', fontsize=11, fontweight='bold')
        ax.set_ylabel('Count', fontsize=11, fontweight='bold')
        ax.set_title('Risk Distribution', fontsize=12, fontweight='bold')
        ax.legend(title='Risk Score', fontsize=8)
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\n‚úì ÂõæË°®Â∑≤‰øùÂ≠ò: {save_path}")
    plt.close()


def generate_text_report(df, output_path):
    """ÁîüÊàêÊñáÊú¨Êä•Âëä"""
    
    total = len(df)
    inside = (df['AD_Status'] == 'Inside (Safe)').sum()
    boundary = (df['AD_Status'] == 'Boundary (Caution)').sum()
    outside = (df['AD_Status'] == 'Outside (High Risk)').sum()
    
    high_risk = df[df['MaxTanimoto'] < 0.4].sort_values('MaxTanimoto')
    
    has_score = 'Total_Score' in df.columns
    
    report = []
    report.append("="*80)
    report.append(f"{PROJECT_NAME}")
    report.append("ÈÄÇÁî®Âüü(AD)ËØäÊñ≠Êä•Âëä")
    report.append("="*80)
    report.append("")
    report.append("## 1. Ê¶ÇËßà")
    report.append(f"   ÊÄªÂàÜÂ≠êÊï∞: {total}")
    report.append(f"   ÂàÜÊûêÊó∂Èó¥: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    report.append("## 2. ADÂàÜÂ∏É")
    report.append(f"   ‚úÖ Inside (Safe):         {inside:6d} ({inside/total*100:5.1f}%)")
    report.append(f"   ‚ö†Ô∏è  Boundary (Caution):    {boundary:6d} ({boundary/total*100:5.1f}%)")
    report.append(f"   üö® Outside (High Risk):   {outside:6d} ({outside/total*100:5.1f}%)")
    report.append("")
    
    report.append("## 3. Áõ∏‰ººÂ∫¶ÁªüËÆ°")
    report.append(f"   Âπ≥Âùá Max Tanimoto: {df['MaxTanimoto'].mean():.4f}")
    report.append(f"   ‰∏≠‰ΩçÊï∞:            {df['MaxTanimoto'].median():.4f}")
    report.append(f"   ÊúÄÂ∞èÂÄº:            {df['MaxTanimoto'].min():.4f}")
    report.append(f"   ÊúÄÂ§ßÂÄº:            {df['MaxTanimoto'].max():.4f}")
    report.append(f"   Ê†áÂáÜÂ∑Æ:            {df['MaxTanimoto'].std():.4f}")
    report.append("")
    
    if has_score:
        report.append("## 4. ScoreÂàÜÊûê")
        report.append(f"   Âπ≥Âùá Total Score:  {df['Total_Score'].mean():.4f}")
        report.append(f"   ‰∏≠‰ΩçÊï∞:            {df['Total_Score'].median():.4f}")
        report.append(f"   ÊúÄÈ´òÂàÜ:            {df['Total_Score'].max():.4f}")
        report.append("")
        
        # È´òÂàÜÈ´òÈ£éÈô©ÂàÜÂ≠ê
        high_score_high_risk = df[(df['Total_Score'] > 0.7) & (df['Risk_Score'] >= 3)]
        report.append(f"   ‚ö†Ô∏è  È´òÂàÜ‰ΩÜÈ´òÈ£éÈô©ÂàÜÂ≠ê: {len(high_score_high_risk)} ‰∏™")
        report.append("      ÔºàËøô‰∫õÂàÜÂ≠êscoreÈ´ò‰ΩÜÂèØËÉΩ‰∏çÂèØÈù†ÔºåÈúÄË∞®ÊÖéÂØπÂæÖÔºâ")
        report.append("")
    
    report.append("## 5. È£éÈô©ËØÑ‰º∞")
    
    outside_pct = outside / total * 100
    if outside_pct > 30:
        report.append(f"   üö® È´òÈ£éÈô©Ë≠¶Âëä: {outside_pct:.1f}% ÂàÜÂ≠êÂú®ADÂ§ñÔºÅ")
        report.append("")
        report.append("   ‚ö†Ô∏è  Âª∫ËÆÆ:")
        report.append("   1. Ëøô‰∫õÂàÜÂ≠êÁöÑQSARÈ¢ÑÊµãÂèØËÉΩÂÆåÂÖ®‰∏çÂáÜÁ°Æ")
        report.append("   2. Âª∫ËÆÆË°•ÂÖÖËÆ≠ÁªÉÊï∞ÊçÆË¶ÜÁõñËøô‰∫õÂåñÂ≠¶Á©∫Èó¥")
        report.append("   3. ÂØπADÂ§ñÁöÑÈ´òÂàÜÂàÜÂ≠êËøõË°åÂÆûÈ™åÈ™åËØÅÂâçË¶ÅÊ†ºÂ§ñË∞®ÊÖé")
        report.append("   4. ËÄÉËôëÂ¢ûÂä†Â§öÊ†∑ÊÄßÊÉ©ÁΩöÂà∞RL rewardÂáΩÊï∞")
    elif outside_pct > 10:
        report.append(f"   ‚ö†Ô∏è  ‰∏≠Á≠âÈ£éÈô©: {outside_pct:.1f}% ÂàÜÂ≠êÂú®ADÂ§ñ")
        report.append("   Âª∫ËÆÆÂØπËøô‰∫õÂàÜÂ≠êÁöÑÈ¢ÑÊµã‰øùÊåÅË∞®ÊÖéÊÄÅÂ∫¶")
    else:
        report.append(f"   ‚úÖ ‰ΩéÈ£éÈô©: ‰ªÖ{outside_pct:.1f}% ÂàÜÂ≠êÂú®ADÂ§ñ")
        report.append("   Â§ßÈÉ®ÂàÜÂàÜÂ≠êÁöÑQSARÈ¢ÑÊµãËæÉ‰∏∫ÂèØÈù†")
    
    report.append("")
    report.append("## 6. È´òÈ£éÈô©ÂàÜÂ≠êËØ¶ÊÉÖ")
    if len(high_risk) > 0:
        report.append(f"   ÂÖ± {len(high_risk)} ‰∏™ÂàÜÂ≠êÁöÑ Max Tanimoto < 0.4")
        report.append("")
        report.append("   ÊúÄÈ´òÈ£éÈô©ÁöÑ10‰∏™ÂàÜÂ≠ê:")
        for i, (idx, row) in enumerate(high_risk.head(10).iterrows(), 1):
            score_info = ""
            if has_score:
                score_info = f"  Score={row['Total_Score']:.3f}"
            report.append(f"   {i:2d}. {row['SMILES'][:55]:55s}  Sim={row['MaxTanimoto']:.4f}{score_info}")
    else:
        report.append("   ‚úÖ Êó†È´òÈ£éÈô©ÂàÜÂ≠ê")
    
    report.append("")
    report.append("="*80)
    report.append("üìä ËØ¶ÁªÜÁªìÊûúÊñá‰ª∂:")
    report.append(f"  - {OUTPUT_DIR}/ad_analysis_results.csv         (ÂÆåÊï¥ADÂàÜÊûê)")
    report.append(f"  - {OUTPUT_DIR}/high_risk_molecules.csv         (È´òÈ£éÈô©ÂàÜÂ≠ê)")
    report.append(f"  - {OUTPUT_DIR}/ad_distribution.png             (ÂèØËßÜÂåñÂõæË°®)")
    
    if has_score:
        report.append(f"  - {OUTPUT_DIR}/high_score_high_risk.csv        (È´òÂàÜÈ´òÈ£éÈô©)")
    
    report.append("="*80)
    
    # ‰øùÂ≠òÊä•Âëä
    report_text = "\n".join(report)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print("\n" + report_text)


# ============================================================================
# ‰∏ªÂáΩÊï∞
# ============================================================================

def main():
    """‰∏ªÊµÅÁ®ã"""
    
    print("="*80)
    print(f"{PROJECT_NAME}")
    print("ÈÄÇÁî®Âüü(AD)ËØäÊñ≠")
    print("="*80)
    print()
    
    # ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Step 1: ËØªÂèñËÆ≠ÁªÉÈõÜ
    print("Step 1/4: ËØªÂèñËÆ≠ÁªÉÈõÜ")
    print("-" * 80)
    training_smiles = load_training_data()
    
    if training_smiles is None or len(training_smiles) == 0:
        print("‚úó Êó†Ê≥ïÂä†ËΩΩËÆ≠ÁªÉÈõÜÔºåÁªàÊ≠¢")
        return
    
    print()
    
    # Step 2: ËØªÂèñÁîüÊàêÂàÜÂ≠ê
    print("Step 2/4: ËØªÂèñÁîüÊàêÂàÜÂ≠ê")
    print("-" * 80)
    generated_smiles, extra_cols = load_generated_data()
    
    if generated_smiles is None or len(generated_smiles) == 0:
        print("‚úó Êó†Ê≥ïÂä†ËΩΩÁîüÊàêÂàÜÂ≠êÔºåÁªàÊ≠¢")
        return
    
    print()
    
    # Step 3: ËÆ°ÁÆóÊåáÁ∫πÂíåADÂàÜÊûê
    print("Step 3/4: ËÆ°ÁÆóÊåáÁ∫πÂíåADÂàÜÊûê")
    print("-" * 80)
    
    train_fps, train_valid = compute_fingerprints(training_smiles)
    print(f"‚úì ËÆ≠ÁªÉÈõÜÊúâÊïàÂàÜÂ≠ê: {len(train_valid)}/{len(training_smiles)}")
    
    gen_fps, gen_valid = compute_fingerprints(generated_smiles)
    print(f"‚úì ÁîüÊàêÈõÜÊúâÊïàÂàÜÂ≠ê: {len(gen_valid)}/{len(generated_smiles)}")
    
    # Â¶ÇÊûúÊúâÈ¢ùÂ§ñÂàóÔºåÈúÄË¶ÅÂØπÂ∫îÁ≠õÈÄâ
    if extra_cols:
        filtered_extra = {}
        valid_indices = [i for i, smi in enumerate(generated_smiles) if Chem.MolFromSmiles(smi) is not None]
        for key, values in extra_cols.items():
            filtered_extra[key] = [values[i] for i in valid_indices]
        extra_cols = filtered_extra
    
    ad_results = calculate_ad_analysis(train_fps, gen_valid, gen_fps, extra_cols)
    
    # ‰øùÂ≠òÁªìÊûú
    result_path = os.path.join(OUTPUT_DIR, "ad_analysis_results.csv")
    ad_results.to_csv(result_path, index=False)
    print(f"\n‚úì ÂÆåÊï¥ÁªìÊûúÂ∑≤‰øùÂ≠ò: {result_path}")
    
    # È´òÈ£éÈô©ÂàÜÂ≠ê
    high_risk = ad_results[ad_results['MaxTanimoto'] < 0.4].sort_values('MaxTanimoto')
    high_risk_path = os.path.join(OUTPUT_DIR, "high_risk_molecules.csv")
    high_risk.to_csv(high_risk_path, index=False)
    print(f"‚úì È´òÈ£éÈô©ÂàÜÂ≠êÂ∑≤‰øùÂ≠ò: {high_risk_path} ({len(high_risk)} ‰∏™)")
    
    # È´òÂàÜÈ´òÈ£éÈô©ÂàÜÂ≠ê
    if 'Total_Score' in ad_results.columns:
        high_score_high_risk = ad_results[
            (ad_results['Total_Score'] > 0.7) & (ad_results['Risk_Score'] >= 3)
        ].sort_values('Total_Score', ascending=False)
        
        hs_hr_path = os.path.join(OUTPUT_DIR, "high_score_high_risk.csv")
        high_score_high_risk.to_csv(hs_hr_path, index=False)
        print(f"‚úì È´òÂàÜÈ´òÈ£éÈô©ÂàÜÂ≠êÂ∑≤‰øùÂ≠ò: {hs_hr_path} ({len(high_score_high_risk)} ‰∏™)")
    
    print()
    
    # Step 4: ÂèØËßÜÂåñÂíåÊä•Âëä
    print("Step 4/4: ÁîüÊàêÂèØËßÜÂåñÂíåÊä•Âëä")
    print("-" * 80)
    
    plot_path = os.path.join(OUTPUT_DIR, "ad_distribution.png")
    plot_ad_distribution(ad_results, plot_path)
    
    report_path = os.path.join(OUTPUT_DIR, "diagnostic_report.txt")
    generate_text_report(ad_results, report_path)
    print(f"‚úì Êä•ÂëäÂ∑≤‰øùÂ≠ò: {report_path}")
    
    print()
    print("="*80)
    print("‚úÖ‚úÖ‚úÖ ËØäÊñ≠ÂÆåÊàêÔºÅ‚úÖ‚úÖ‚úÖ")
    print("="*80)
    print(f"\nüìÅ ÊâÄÊúâÁªìÊûú‰øùÂ≠òÂú®: {OUTPUT_DIR}/")
    print("\nüîç ÂÖ≥ÈîÆÂèëÁé∞:")
    
    inside = (ad_results['AD_Status'] == 'Inside (Safe)').sum()
    outside = (ad_results['AD_Status'] == 'Outside (High Risk)').sum()
    total = len(ad_results)
    
    print(f"   ‚Ä¢ {inside}/{total} ({inside/total*100:.1f}%) ÂàÜÂ≠êÂú®ADÂÜÖÔºàÂèØÈù†Ôºâ")
    print(f"   ‚Ä¢ {outside}/{total} ({outside/total*100:.1f}%) ÂàÜÂ≠êÂú®ADÂ§ñÔºàÈ´òÈ£éÈô©Ôºâ")
    
    if 'Total_Score' in ad_results.columns:
        high_score_count = (ad_results['Total_Score'] > 0.7).sum()
        print(f"   ‚Ä¢ {high_score_count} ‰∏™È´òÂàÜÂàÜÂ≠ê (Score > 0.7)")
    
    print()


if __name__ == "__main__":
    main()