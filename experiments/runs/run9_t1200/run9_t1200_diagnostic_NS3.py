"""
Run9_t1200 ä¸“ç”¨è¯Šæ–­è„šæœ¬
é€‚ç”¨åŸŸ(AD)åˆ†æ - æ£€æµ‹ç”Ÿæˆåˆ†å­æ˜¯å¦å¯é 
"""

import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import os

# ============================================================================
# é…ç½®åŒº - æ ¹æ®æ‚¨çš„å®é™…æƒ…å†µå·²é…ç½®å¥½
# ============================================================================

# è®­ç»ƒé›†æ–‡ä»¶ï¼ˆå®é™…ä½¿ç”¨çš„æ¸…æ´ç‰ˆæœ¬ï¼‰
TRAINING_FILE = None  # ç”¨åŸå§‹æ•°æ®
BACKUP_TRAINING_FILE = "../../../data/NS3.csv"


# ç”Ÿæˆåˆ†å­æ–‡ä»¶
GENERATED_FILE = "results_1.csv"

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "diagnostic_results"

# é¡¹ç›®åç§°
PROJECT_NAME = "Run9_t1200 - DENV NS3 Inhibitor Generation"

# ============================================================================
# æ•°æ®è¯»å–å‡½æ•°
# ============================================================================

def load_training_data():
    """æ™ºèƒ½åŠ è½½è®­ç»ƒé›†æ•°æ®"""
    
    print("æ­£åœ¨åŠ è½½è®­ç»ƒé›†...")
    
    print(f"è¯»å–æ–‡ä»¶: {BACKUP_TRAINING_FILE}")
    try:
        # æŒ‡å®šencodingå¤„ç†BOM
        df = pd.read_csv(BACKUP_TRAINING_FILE, encoding='utf-8-sig')
        
        print(f"âœ“ æ–‡ä»¶å½¢çŠ¶: {df.shape}")
        print(f"  å‰5åˆ—: {df.columns.tolist()[:5]}")
        
        # ç¡®è®¤Smilesåˆ—å­˜åœ¨
        if 'Smiles' not in df.columns:
            print(f"âœ— é”™è¯¯: æ‰¾ä¸åˆ°Smilesåˆ—")
            print(f"  å¯ç”¨åˆ—: {df.columns.tolist()}")
            return None
        
        smiles_list = df['Smiles'].dropna().tolist()
        print(f"âœ“ è¯»å–äº† {len(smiles_list)} ä¸ªSMILES")
        print(f"  ç¤ºä¾‹: {smiles_list[0][:60]}...")
        
        # ğŸ”´ å…³é”®ï¼šåªåšåŸºæœ¬æ¸…æ´—ï¼Œä¸è¦è¶…ä¸¥æ ¼ç­›é€‰
        clean_smiles = []
        
        for smi in tqdm(smiles_list, desc="æ¸…æ´—è®­ç»ƒé›†"):
            if pd.isna(smi):
                continue
            
            smi_str = str(smi).strip()
            
            # è·³è¿‡ç©ºå­—ç¬¦ä¸²
            if not smi_str:
                continue
            
            try:
                mol = Chem.MolFromSmiles(smi_str)
                if mol is not None:
                    canonical_smiles = Chem.MolToSmiles(mol)
                    
                    # åªè¿‡æ»¤æ˜æ˜¾é”™è¯¯çš„ï¼š
                    # 1. å¤šç»„åˆ†åˆ†å­ï¼ˆå«.ï¼‰
                    # 2. è¿‡é•¿çš„SMILESï¼ˆå¯èƒ½æœ‰é”™è¯¯ï¼‰
                    if '.' not in canonical_smiles and len(canonical_smiles) < 300:
                        clean_smiles.append(canonical_smiles)
            except:
                continue
        
        # å»é‡
        unique_smiles = list(dict.fromkeys(clean_smiles))
        
        print(f"âœ“ æ¸…æ´—å: {len(unique_smiles)} ä¸ªæœ‰æ•ˆSMILES")
        
        if len(unique_smiles) == 0:
            print("âœ— é”™è¯¯: æ¸…æ´—åæ²¡æœ‰æœ‰æ•ˆåˆ†å­ï¼")
            return None
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªåˆ†å­ç¡®è®¤
        print(f"  ç¬¬1ä¸ª: {unique_smiles[0][:60]}...")
        
        return unique_smiles
        
    except Exception as e:
        print(f"âœ— è¯»å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def apply_ultra_clean_filter(smiles_list):
    """åº”ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„è¶…ä¸¥æ ¼ç­›é€‰"""
    
    def is_ultra_compatible(smiles):
        forbidden_tokens = ['[n-]', '[s+]', '[N-]', '[S-]', '[O+]', '[C-]', '[c-]']
        
        for token in forbidden_tokens:
            if token in smiles:
                return False
        
        if any(x in smiles for x in ['[nH+]', '[s+]', '[o+]']):
            return False
        
        if any(x in smiles for x in ['@', '/', '\\']):
            return False
        
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is None:
                return False
            
            if mol.GetNumAtoms() > 50:
                return False
            
            atoms = set([atom.GetSymbol() for atom in mol.GetAtoms()])
            allowed_atoms = {'C', 'N', 'O', 'S', 'F', 'Cl', 'Br', 'H'}
            if not atoms.issubset(allowed_atoms):
                return False
            
            return True
        except:
            return False
    
    valid_smiles = []
    
    for smiles in tqdm(smiles_list, desc="ç­›é€‰è®­ç»ƒé›†"):
        if pd.isna(smiles):
            continue
        
        smiles_str = str(smiles).strip()
        
        try:
            mol = Chem.MolFromSmiles(smiles_str)
            if mol is not None:
                canonical_smiles = Chem.MolToSmiles(mol)
                
                if ('.' not in canonical_smiles and 
                    '|' not in canonical_smiles and
                    len(canonical_smiles) < 100 and
                    is_ultra_compatible(canonical_smiles)):
                    valid_smiles.append(canonical_smiles)
        except:
            continue
    
    # å»é‡
    unique_smiles = list(dict.fromkeys(valid_smiles))
    return unique_smiles


def load_generated_data():
    """åŠ è½½ç”Ÿæˆçš„åˆ†å­æ•°æ®"""
    
    print(f"\næ­£åœ¨åŠ è½½ç”Ÿæˆåˆ†å­: {GENERATED_FILE}")
    
    try:
        # æ˜ç¡®æŒ‡å®šé€—å·åˆ†éš”
        df = pd.read_csv(GENERATED_FILE, sep=',')
        
        print(f"  æ–‡ä»¶å½¢çŠ¶: {df.shape}")
        print(f"  å‰5åˆ—: {df.columns.tolist()[:5]}")
        
        # ç›´æ¥ä½¿ç”¨SMILESåˆ—
        if 'SMILES' not in df.columns:
            print(f"  âœ— é”™è¯¯: æœªæ‰¾åˆ°SMILESåˆ—")
            print(f"  å¯ç”¨åˆ—: {df.columns.tolist()}")
            return None, None
        
        smiles_list = df['SMILES'].dropna().tolist()
        print(f"âœ“ åŠ è½½äº† {len(smiles_list)} ä¸ªç”Ÿæˆåˆ†å­")
        print(f"  ç¬¬1ä¸ª: {smiles_list[0][:60]}...")
        
        # è¯»å–é¢å¤–ä¿¡æ¯
        extra_cols = {}
        if 'Score' in df.columns:
            extra_cols['total_score'] = df['Score'].tolist()
            print(f"  âœ“ æ‰¾åˆ°Scoreåˆ—")
        if 'DENV_Activity (raw)' in df.columns:
            extra_cols['DENV_Activity'] = df['DENV_Activity (raw)'].tolist()
            print(f"  âœ“ æ‰¾åˆ°DENV_Activityåˆ—")
        if 'NLL' in df.columns:
            extra_cols['NLL'] = df['NLL'].tolist()
        
        return smiles_list, extra_cols
        
    except Exception as e:
        print(f"âœ— è¯»å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None


# ============================================================================
# ADåˆ†ææ ¸å¿ƒå‡½æ•°
# ============================================================================

def compute_fingerprints(smiles_list, radius=2, n_bits=2048):
    """è®¡ç®—MorganæŒ‡çº¹"""
    fps = []
    valid_smiles = []
    
    for smi in tqdm(smiles_list, desc="è®¡ç®—æŒ‡çº¹"):
        mol = Chem.MolFromSmiles(smi)
        if mol:
            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)
            fps.append(fp)
            valid_smiles.append(smi)
    
    return fps, valid_smiles


def calculate_ad_analysis(training_fps, generated_smiles, generated_fps, extra_cols=None):
    """è®¡ç®—ADåˆ†æ"""
    results = []
    
    print("\nè®¡ç®—ADè·ç¦»...")
    for idx, (smi, qfp) in enumerate(tqdm(zip(generated_smiles, generated_fps), 
                                          total=len(generated_smiles))):
        # è®¡ç®—åˆ°æ‰€æœ‰è®­ç»ƒé›†åˆ†å­çš„Tanimotoç›¸ä¼¼åº¦
        similarities = [
            DataStructs.TanimotoSimilarity(qfp, tfp) 
            for tfp in training_fps
        ]
        
        max_sim = np.max(similarities)
        mean_sim = np.mean(similarities)
        
        # ADåˆ¤å®š
        if max_sim > 0.6:
            status = "Inside (Safe)"
            risk_score = 1
        elif max_sim > 0.4:
            status = "Boundary (Caution)"
            risk_score = 3
        else:
            status = "Outside (High Risk)"
            risk_score = 5
        
        result = {
            'SMILES': smi,
            'MaxTanimoto': max_sim,
            'MeanTanimoto': mean_sim,
            'AD_Status': status,
            'Risk_Score': risk_score
        }
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        if extra_cols:
            if 'total_score' in extra_cols:
                result['Total_Score'] = extra_cols['total_score'][idx]
            if 'NLL' in extra_cols:
                result['NLL'] = extra_cols['NLL'][idx]
        
        results.append(result)
    
    df = pd.DataFrame(results)
    
    # ç»Ÿè®¡æŠ¥å‘Š
    print("\n" + "="*70)
    print("é€‚ç”¨åŸŸ(AD)åˆ†ææŠ¥å‘Š")
    print("="*70)
    print(f"æ€»åˆ†å­æ•°: {len(df)}")
    print(f"\nADåˆ†å¸ƒ:")
    for status in df['AD_Status'].value_counts().items():
        print(f"  {status[0]}: {status[1]} ({status[1]/len(df)*100:.1f}%)")
    print(f"\næœ€å¤§Tanimotoç»Ÿè®¡:")
    print(df['MaxTanimoto'].describe())
    
    if 'Total_Score' in df.columns:
        print(f"\nTotal Scoreç»Ÿè®¡:")
        print(df['Total_Score'].describe())
        
        # é«˜åˆ†ä½†é«˜é£é™©çš„åˆ†å­
        high_score_high_risk = df[(df['Total_Score'] > 0.7) & (df['Risk_Score'] >= 3)]
        print(f"\né«˜åˆ†ä½†é«˜é£é™©åˆ†å­: {len(high_score_high_risk)} ä¸ª")
    
    print("="*70)
    
    return df


def plot_ad_distribution(df, save_path):
    """å¯è§†åŒ–ADåˆ†å¸ƒ"""
    
    # åˆ¤æ–­æ˜¯å¦æœ‰scoreä¿¡æ¯
    has_score = 'Total_Score' in df.columns
    
    if has_score:
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        axes = axes.flatten()
    else:
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
    
    # 1. Tanimotoåˆ†å¸ƒç›´æ–¹å›¾
    ax = axes[0]
    ax.hist(df['MaxTanimoto'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')
    ax.axvline(x=0.6, color='green', linestyle='--', linewidth=2, label='Safe (>0.6)')
    ax.axvline(x=0.4, color='orange', linestyle='--', linewidth=2, label='Caution (0.4-0.6)')
    ax.set_xlabel('Max Tanimoto Similarity', fontsize=11, fontweight='bold')
    ax.set_ylabel('Count', fontsize=11, fontweight='bold')
    ax.set_title('AD Distribution', fontsize=12, fontweight='bold')
    ax.legend(fontsize=9)
    ax.grid(alpha=0.3)
    
    # 2. ADçŠ¶æ€é¥¼å›¾
    ax = axes[1]
    status_counts = df['AD_Status'].value_counts()
    colors = {'Inside (Safe)': '#2ecc71', 
              'Boundary (Caution)': '#f39c12', 
              'Outside (High Risk)': '#e74c3c'}
    ax.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%',
            colors=[colors.get(s, 'gray') for s in status_counts.index], 
            startangle=90, textprops={'fontsize': 9, 'fontweight': 'bold'})
    ax.set_title('AD Status', fontsize=12, fontweight='bold')
    
    # 3. ç´¯ç§¯åˆ†å¸ƒ
    ax = axes[2]
    sorted_sim = np.sort(df['MaxTanimoto'])
    cumulative = np.arange(1, len(sorted_sim) + 1) / len(sorted_sim)
    ax.plot(sorted_sim, cumulative, linewidth=2, color='darkblue')
    ax.axvline(x=0.4, color='orange', linestyle='--', alpha=0.7, linewidth=2)
    ax.axvline(x=0.6, color='green', linestyle='--', alpha=0.7, linewidth=2)
    ax.set_xlabel('Max Tanimoto', fontsize=11, fontweight='bold')
    ax.set_ylabel('Cumulative Probability', fontsize=11, fontweight='bold')
    ax.set_title('Cumulative Distribution', fontsize=12, fontweight='bold')
    ax.grid(alpha=0.3)
    
    # 4. Max vs Mean Tanimoto
    ax = axes[3]
    scatter = ax.scatter(df['MeanTanimoto'], df['MaxTanimoto'], 
                         c=df['MaxTanimoto'], cmap='RdYlGn', 
                         alpha=0.6, s=20, edgecolors='black', linewidth=0.5)
    ax.axhline(y=0.6, color='green', linestyle='--', alpha=0.5, linewidth=2)
    ax.axhline(y=0.4, color='orange', linestyle='--', alpha=0.5, linewidth=2)
    ax.set_xlabel('Mean Tanimoto', fontsize=11, fontweight='bold')
    ax.set_ylabel('Max Tanimoto', fontsize=11, fontweight='bold')
    ax.set_title('Mean vs Max Tanimoto', fontsize=12, fontweight='bold')
    plt.colorbar(scatter, ax=ax, label='Max Tanimoto')
    ax.grid(alpha=0.3)
    
    if has_score:
        # 5. Score vs AD
        ax = axes[4]
        for status in df['AD_Status'].unique():
            subset = df[df['AD_Status'] == status]
            ax.scatter(subset['MaxTanimoto'], subset['Total_Score'], 
                      label=status, alpha=0.6, s=30)
        ax.set_xlabel('Max Tanimoto', fontsize=11, fontweight='bold')
        ax.set_ylabel('Total Score', fontsize=11, fontweight='bold')
        ax.set_title('Score vs AD', fontsize=12, fontweight='bold')
        ax.legend(fontsize=8)
        ax.grid(alpha=0.3)
        
        # 6. Risk Scoreåˆ†å¸ƒ
        ax = axes[5]
        risk_counts = df.groupby(['AD_Status', 'Risk_Score']).size().unstack(fill_value=0)
        risk_counts.plot(kind='bar', stacked=True, ax=ax, 
                        color=['#2ecc71', '#f39c12', '#e74c3c'])
        ax.set_xlabel('AD Status', fontsize=11, fontweight='bold')
        ax.set_ylabel('Count', fontsize=11, fontweight='bold')
        ax.set_title('Risk Distribution', fontsize=12, fontweight='bold')
        ax.legend(title='Risk Score', fontsize=8)
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ“ å›¾è¡¨å·²ä¿å­˜: {save_path}")
    plt.close()


def generate_text_report(df, output_path):
    """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
    
    total = len(df)
    inside = (df['AD_Status'] == 'Inside (Safe)').sum()
    boundary = (df['AD_Status'] == 'Boundary (Caution)').sum()
    outside = (df['AD_Status'] == 'Outside (High Risk)').sum()
    
    high_risk = df[df['MaxTanimoto'] < 0.4].sort_values('MaxTanimoto')
    
    has_score = 'Total_Score' in df.columns
    
    report = []
    report.append("="*80)
    report.append(f"{PROJECT_NAME}")
    report.append("é€‚ç”¨åŸŸ(AD)è¯Šæ–­æŠ¥å‘Š")
    report.append("="*80)
    report.append("")
    report.append("## 1. æ¦‚è§ˆ")
    report.append(f"   æ€»åˆ†å­æ•°: {total}")
    report.append(f"   åˆ†ææ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    report.append("## 2. ADåˆ†å¸ƒ")
    report.append(f"   âœ… Inside (Safe):         {inside:6d} ({inside/total*100:5.1f}%)")
    report.append(f"   âš ï¸  Boundary (Caution):    {boundary:6d} ({boundary/total*100:5.1f}%)")
    report.append(f"   ğŸš¨ Outside (High Risk):   {outside:6d} ({outside/total*100:5.1f}%)")
    report.append("")
    
    report.append("## 3. ç›¸ä¼¼åº¦ç»Ÿè®¡")
    report.append(f"   å¹³å‡ Max Tanimoto: {df['MaxTanimoto'].mean():.4f}")
    report.append(f"   ä¸­ä½æ•°:            {df['MaxTanimoto'].median():.4f}")
    report.append(f"   æœ€å°å€¼:            {df['MaxTanimoto'].min():.4f}")
    report.append(f"   æœ€å¤§å€¼:            {df['MaxTanimoto'].max():.4f}")
    report.append(f"   æ ‡å‡†å·®:            {df['MaxTanimoto'].std():.4f}")
    report.append("")
    
    if has_score:
        report.append("## 4. Scoreåˆ†æ")
        report.append(f"   å¹³å‡ Total Score:  {df['Total_Score'].mean():.4f}")
        report.append(f"   ä¸­ä½æ•°:            {df['Total_Score'].median():.4f}")
        report.append(f"   æœ€é«˜åˆ†:            {df['Total_Score'].max():.4f}")
        report.append("")
        
        # é«˜åˆ†é«˜é£é™©åˆ†å­
        high_score_high_risk = df[(df['Total_Score'] > 0.7) & (df['Risk_Score'] >= 3)]
        report.append(f"   âš ï¸  é«˜åˆ†ä½†é«˜é£é™©åˆ†å­: {len(high_score_high_risk)} ä¸ª")
        report.append("      ï¼ˆè¿™äº›åˆ†å­scoreé«˜ä½†å¯èƒ½ä¸å¯é ï¼Œéœ€è°¨æ…å¯¹å¾…ï¼‰")
        report.append("")
    
    report.append("## 5. é£é™©è¯„ä¼°")
    
    outside_pct = outside / total * 100
    if outside_pct > 30:
        report.append(f"   ğŸš¨ é«˜é£é™©è­¦å‘Š: {outside_pct:.1f}% åˆ†å­åœ¨ADå¤–ï¼")
        report.append("")
        report.append("   âš ï¸  å»ºè®®:")
        report.append("   1. è¿™äº›åˆ†å­çš„QSARé¢„æµ‹å¯èƒ½å®Œå…¨ä¸å‡†ç¡®")
        report.append("   2. å»ºè®®è¡¥å……è®­ç»ƒæ•°æ®è¦†ç›–è¿™äº›åŒ–å­¦ç©ºé—´")
        report.append("   3. å¯¹ADå¤–çš„é«˜åˆ†åˆ†å­è¿›è¡Œå®éªŒéªŒè¯å‰è¦æ ¼å¤–è°¨æ…")
        report.append("   4. è€ƒè™‘å¢åŠ å¤šæ ·æ€§æƒ©ç½šåˆ°RL rewardå‡½æ•°")
    elif outside_pct > 10:
        report.append(f"   âš ï¸  ä¸­ç­‰é£é™©: {outside_pct:.1f}% åˆ†å­åœ¨ADå¤–")
        report.append("   å»ºè®®å¯¹è¿™äº›åˆ†å­çš„é¢„æµ‹ä¿æŒè°¨æ…æ€åº¦")
    else:
        report.append(f"   âœ… ä½é£é™©: ä»…{outside_pct:.1f}% åˆ†å­åœ¨ADå¤–")
        report.append("   å¤§éƒ¨åˆ†åˆ†å­çš„QSARé¢„æµ‹è¾ƒä¸ºå¯é ")
    
    report.append("")
    report.append("## 6. é«˜é£é™©åˆ†å­è¯¦æƒ…")
    if len(high_risk) > 0:
        report.append(f"   å…± {len(high_risk)} ä¸ªåˆ†å­çš„ Max Tanimoto < 0.4")
        report.append("")
        report.append("   æœ€é«˜é£é™©çš„10ä¸ªåˆ†å­:")
        for i, (idx, row) in enumerate(high_risk.head(10).iterrows(), 1):
            score_info = ""
            if has_score:
                score_info = f"  Score={row['Total_Score']:.3f}"
            report.append(f"   {i:2d}. {row['SMILES'][:55]:55s}  Sim={row['MaxTanimoto']:.4f}{score_info}")
    else:
        report.append("   âœ… æ— é«˜é£é™©åˆ†å­")
    
    report.append("")
    report.append("="*80)
    report.append("ğŸ“Š è¯¦ç»†ç»“æœæ–‡ä»¶:")
    report.append(f"  - {OUTPUT_DIR}/ad_analysis_results.csv         (å®Œæ•´ADåˆ†æ)")
    report.append(f"  - {OUTPUT_DIR}/high_risk_molecules.csv         (é«˜é£é™©åˆ†å­)")
    report.append(f"  - {OUTPUT_DIR}/ad_distribution.png             (å¯è§†åŒ–å›¾è¡¨)")
    
    if has_score:
        report.append(f"  - {OUTPUT_DIR}/high_score_high_risk.csv        (é«˜åˆ†é«˜é£é™©)")
    
    report.append("="*80)
    
    # ä¿å­˜æŠ¥å‘Š
    report_text = "\n".join(report)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print("\n" + report_text)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»æµç¨‹"""
    
    print("="*80)
    print(f"{PROJECT_NAME}")
    print("é€‚ç”¨åŸŸ(AD)è¯Šæ–­")
    print("="*80)
    print()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Step 1: è¯»å–è®­ç»ƒé›†
    print("Step 1/4: è¯»å–è®­ç»ƒé›†")
    print("-" * 80)
    training_smiles = load_training_data()
    
    if training_smiles is None or len(training_smiles) == 0:
        print("âœ— æ— æ³•åŠ è½½è®­ç»ƒé›†ï¼Œç»ˆæ­¢")
        return
    
    print()
    
    # Step 2: è¯»å–ç”Ÿæˆåˆ†å­
    print("Step 2/4: è¯»å–ç”Ÿæˆåˆ†å­")
    print("-" * 80)
    generated_smiles, extra_cols = load_generated_data()
    
    if generated_smiles is None or len(generated_smiles) == 0:
        print("âœ— æ— æ³•åŠ è½½ç”Ÿæˆåˆ†å­ï¼Œç»ˆæ­¢")
        return
    
    print()
    
    # Step 3: è®¡ç®—æŒ‡çº¹å’ŒADåˆ†æ
    print("Step 3/4: è®¡ç®—æŒ‡çº¹å’ŒADåˆ†æ")
    print("-" * 80)
    
    train_fps, train_valid = compute_fingerprints(training_smiles)
    print(f"âœ“ è®­ç»ƒé›†æœ‰æ•ˆåˆ†å­: {len(train_valid)}/{len(training_smiles)}")
    
    gen_fps, gen_valid = compute_fingerprints(generated_smiles)
    print(f"âœ“ ç”Ÿæˆé›†æœ‰æ•ˆåˆ†å­: {len(gen_valid)}/{len(generated_smiles)}")
    
    # å¦‚æœæœ‰é¢å¤–åˆ—ï¼Œéœ€è¦å¯¹åº”ç­›é€‰
    if extra_cols:
        filtered_extra = {}
        valid_indices = [i for i, smi in enumerate(generated_smiles) if Chem.MolFromSmiles(smi) is not None]
        for key, values in extra_cols.items():
            filtered_extra[key] = [values[i] for i in valid_indices]
        extra_cols = filtered_extra
    
    ad_results = calculate_ad_analysis(train_fps, gen_valid, gen_fps, extra_cols)
    
    # ä¿å­˜ç»“æœ
    result_path = os.path.join(OUTPUT_DIR, "ad_analysis_results.csv")
    ad_results.to_csv(result_path, index=False)
    print(f"\nâœ“ å®Œæ•´ç»“æœå·²ä¿å­˜: {result_path}")
    
    # é«˜é£é™©åˆ†å­
    high_risk = ad_results[ad_results['MaxTanimoto'] < 0.4].sort_values('MaxTanimoto')
    high_risk_path = os.path.join(OUTPUT_DIR, "high_risk_molecules.csv")
    high_risk.to_csv(high_risk_path, index=False)
    print(f"âœ“ é«˜é£é™©åˆ†å­å·²ä¿å­˜: {high_risk_path} ({len(high_risk)} ä¸ª)")
    
    # é«˜åˆ†é«˜é£é™©åˆ†å­
    if 'Total_Score' in ad_results.columns:
        high_score_high_risk = ad_results[
            (ad_results['Total_Score'] > 0.7) & (ad_results['Risk_Score'] >= 3)
        ].sort_values('Total_Score', ascending=False)
        
        hs_hr_path = os.path.join(OUTPUT_DIR, "high_score_high_risk.csv")
        high_score_high_risk.to_csv(hs_hr_path, index=False)
        print(f"âœ“ é«˜åˆ†é«˜é£é™©åˆ†å­å·²ä¿å­˜: {hs_hr_path} ({len(high_score_high_risk)} ä¸ª)")
    
    print()
    
    # Step 4: å¯è§†åŒ–å’ŒæŠ¥å‘Š
    print("Step 4/4: ç”Ÿæˆå¯è§†åŒ–å’ŒæŠ¥å‘Š")
    print("-" * 80)
    
    plot_path = os.path.join(OUTPUT_DIR, "ad_distribution.png")
    plot_ad_distribution(ad_results, plot_path)
    
    report_path = os.path.join(OUTPUT_DIR, "diagnostic_report.txt")
    generate_text_report(ad_results, report_path)
    print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    print()
    print("="*80)
    print("âœ…âœ…âœ… è¯Šæ–­å®Œæˆï¼âœ…âœ…âœ…")
    print("="*80)
    print(f"\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}/")
    print("\nğŸ” å…³é”®å‘ç°:")
    
    inside = (ad_results['AD_Status'] == 'Inside (Safe)').sum()
    outside = (ad_results['AD_Status'] == 'Outside (High Risk)').sum()
    total = len(ad_results)
    
    print(f"   â€¢ {inside}/{total} ({inside/total*100:.1f}%) åˆ†å­åœ¨ADå†…ï¼ˆå¯é ï¼‰")
    print(f"   â€¢ {outside}/{total} ({outside/total*100:.1f}%) åˆ†å­åœ¨ADå¤–ï¼ˆé«˜é£é™©ï¼‰")
    
    if 'Total_Score' in ad_results.columns:
        high_score_count = (ad_results['Total_Score'] > 0.7).sum()
        print(f"   â€¢ {high_score_count} ä¸ªé«˜åˆ†åˆ†å­ (Score > 0.7)")
    
    print()


if __name__ == "__main__":
    main()